{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTOl82_NaJDi",
        "outputId": "4cd4ccaa-2361-4137-d774-e81a65533c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Epoch 1: 100%|██████████| 6758/6758 [05:18<00:00, 21.19it/s, loss=0.612]\n",
            "Epoch 2: 100%|██████████| 6758/6758 [05:20<00:00, 21.11it/s, loss=0.802]\n",
            "Epoch 3: 100%|██████████| 6758/6758 [05:18<00:00, 21.24it/s, loss=nan]\n",
            "Epoch 4: 100%|██████████| 6758/6758 [05:16<00:00, 21.35it/s, loss=4.31]\n",
            "Epoch 5: 100%|██████████| 6758/6758 [05:17<00:00, 21.31it/s, loss=2.16]\n",
            "Epoch 6: 100%|██████████| 6758/6758 [05:17<00:00, 21.31it/s, loss=2.14]\n",
            "Epoch 7: 100%|██████████| 6758/6758 [05:16<00:00, 21.36it/s, loss=nan]\n",
            "Epoch 8: 100%|██████████| 6758/6758 [05:16<00:00, 21.36it/s, loss=1.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Pseudocode:\n",
            " ((arri high,, 1, high)(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load Dataset\n",
        "df = pd.read_csv(\"spoc-train.csv\")\n",
        "df = df.dropna()\n",
        "\n",
        "# Save C++ & pseudocode pairs for tokenizer training\n",
        "cpp_file = \"cpp.txt\"\n",
        "pseudo_file = \"pseudocode.txt\"\n",
        "df[\"code\"].to_csv(cpp_file, index=False, header=False)\n",
        "df[\"text\"].to_csv(pseudo_file, index=False, header=False)\n",
        "\n",
        "# Train BPE Tokenizer with a moderate vocabulary size\n",
        "spm.SentencePieceTrainer.train(input=f\"{cpp_file},{pseudo_file}\", model_prefix=\"bpe\", vocab_size=8000)\n",
        "\n",
        "# Load Tokenizer\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"bpe.model\")\n",
        "\n",
        "# Dataset Class\n",
        "class CodeDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.data = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cpp = self.data.iloc[idx][\"code\"]\n",
        "        pseudo = self.data.iloc[idx][\"text\"]\n",
        "        cpp_ids = self.tokenizer.encode(cpp, out_type=int)\n",
        "        pseudo_ids = self.tokenizer.encode(pseudo, out_type=int)\n",
        "        return torch.tensor(cpp_ids), torch.tensor(pseudo_ids)\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = CodeDataset(df, sp)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: x)\n",
        "\n",
        "# Transformer Model with moderate capacity\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src).permute(1, 0, 2)\n",
        "        tgt = self.embedding(tgt).permute(1, 0, 2)\n",
        "        output = self.transformer(src, tgt)\n",
        "        return self.fc_out(output).permute(1, 0, 2)\n",
        "\n",
        "# Training Setup\n",
        "vocab_size = 16000\n",
        "model = TransformerModel(vocab_size).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "def train_model(model, dataloader, epochs=8):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
        "        epoch_loss = 0\n",
        "        for batch in loop:\n",
        "            cpp_batch, pseudo_batch = zip(*batch)\n",
        "            cpp_batch = nn.utils.rnn.pad_sequence(cpp_batch, batch_first=True).long().to(\"cuda\")\n",
        "            pseudo_batch = nn.utils.rnn.pad_sequence(pseudo_batch, batch_first=True).long().to(\"cuda\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(cpp_batch, pseudo_batch[:, :-1])\n",
        "            loss = criterion(output.reshape(-1, vocab_size), pseudo_batch[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Adjust learning rate based on epoch loss\n",
        "        scheduler.step(epoch_loss / len(dataloader))\n",
        "\n",
        "train_model(model, dataloader)\n",
        "\n",
        "# Save Model & Tokenizer\n",
        "torch.save(model.state_dict(), \"transformer_model.pth\")\n",
        "\n",
        "# Save the tokenizer model\n",
        "with open(\"bpe.model\", \"wb\") as f:\n",
        "    f.write(sp.serialized_model_proto())\n",
        "\n",
        "# Testing\n",
        "def generate(model, tokenizer, code):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        code_ids = tokenizer.encode(code, out_type=int)\n",
        "        code_tensor = torch.tensor(code_ids).unsqueeze(0).to(\"cuda\")\n",
        "        output = model(code_tensor, code_tensor)\n",
        "        predicted_ids = torch.argmax(output, dim=-1).squeeze().tolist()\n",
        "        return tokenizer.decode(predicted_ids)\n",
        "\n",
        "# Example: Generate pseudocode from C++ code\n",
        "sample_code = \"\"\"\n",
        "        quicksort(arr, pi + 1, high);\n",
        "\"\"\"\n",
        "predicted_pseudo = generate(model, sp, sample_code)\n",
        "print(\"Generated Pseudocode:\\n\", predicted_pseudo)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Generate pseudocode from C++ code\n",
        "sample_code = \"\"\"\n",
        "        int x;\n",
        "        if (n >= 5)\n",
        "        {\n",
        "          cout\"number is greater\";\n",
        "        }\n",
        "\"\"\"\n",
        "predicted_pseudo = generate(model, sp, sample_code)\n",
        "print(\"Generated Pseudocode:\\n\", predicted_pseudo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOb90xiakMvJ",
        "outputId": "4a23d6ec-944c-453a-a462-d398a935e0eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Pseudocode:\n",
            "  ⁇  is ⁇  (n\" 5);\" (; is greater than; ⁇ n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bdLtXfaQkX89"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}